{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a49e9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "415baf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu118\n",
      "True\n",
      "NVIDIA GeForce RTX 2060\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e8b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 9900\n",
      "Sample data: ('function transfer ( address _to , uint256 _value ) public returns ( bool success ) ; function transferFrom ( address _from , address _to , uint256 _value ) public returns ( bool success ) ; function approve ( address _spender , uint256 _value ) public returns ( bool success ) ; function allowance ( address _owner , address _spender ) public constant returns ( uint256 remaining ) ; event Transfer ( address indexed _from , address indexed _to , uint256 _value ) ; event Approval ( address indexed _owner , address indexed _spender , uint256 _value ) ; } contract GTO is ERC20Interface { uint8 public constant decimals = 5 ; string public constant symbol = GTO ; string public constant name = GTO ; bool public _selling = false ; uint256 public _totalSupply = 10 * * 14 ; uint256 public _originalBuyPrice = 45 * 10 * * 7 ; address public owner ; mapping ( address = > uint256 ) private balances ; mapping ( address = > mapping ( address = > uint256 ) ) private allowed ; mapping ( address = > bool ) private approvedInvestorList ; mapping ( address = > uint256 ) private deposit ; address [ ] private buyers ; uint8 public _icoPercent = 10 ; uint256 public _icoSupply = _totalSupply * _icoPercent / 100 ; uint256 public _minimumBuy = 3 * 10 * * 17 ; uint256 public _maximumBuy = 30 * 10 * * 18 ; modifier onlyOwner ( ) { require ( msg . sender = = owner ) ; _ ; }', 0)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Kết nối tới MongoDB và truy vấn dữ liệu\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['Interaction_and_Contract_State_Vulnerabilities']\n",
    "\n",
    "vulnerabilities = ['reentrancy', 'delegatecall', 'unchecked_external_call', 'unchecked_send']\n",
    "labels = {vulnerability: i for i, vulnerability in enumerate(vulnerabilities)}\n",
    "data = []\n",
    "\n",
    "def fetch_data_from_collection(collection_name, label):\n",
    "    collection = db[collection_name]\n",
    "    documents = collection.find({})\n",
    "    for doc in documents:\n",
    "        if 'extract_feature' in doc:\n",
    "            for feature in doc['extract_feature']:\n",
    "                tokens = feature.get('tokens', [])\n",
    "                if tokens:\n",
    "                    data.append((' '.join(tokens), label))\n",
    "\n",
    "for vulnerability in vulnerabilities:\n",
    "    fetch_data_from_collection(vulnerability, labels[vulnerability])\n",
    "\n",
    "print(f\"Number of samples: {len(data)}\")\n",
    "if data:\n",
    "    print(f\"Sample data: {data[8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a27943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Định nghĩa lớp SolidityDataset\n",
    "class SolidityDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text, label = self.data[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dfb1f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Chuẩn bị dữ liệu và dataloader\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "if data:\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train_dataset = SolidityDataset(train_data, tokenizer, MAX_LEN)\n",
    "    test_dataset = SolidityDataset(test_data, tokenizer, MAX_LEN)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "else:\n",
    "    print(\"No data available to split into training and test sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1211e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33515aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Khởi tạo mô hình và các tham số huấn luyện\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Định nghĩa các siêu tham số của mô hình BERT\n",
    "config = BertConfig(\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act='gelu',\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    pad_token_id=0,\n",
    "    gradient_checkpointing=False,\n",
    "    position_embedding_type='absolute',\n",
    "    use_cache=True,\n",
    "    classifier_dropout=None,\n",
    "    num_labels=len(vulnerabilities)\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "model = model.to(device)\n",
    "\n",
    "EPOCHS = 13\n",
    "LEARNING_RATE = 2e-5\n",
    "EPSILON = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3944f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Định nghĩa hàm train_epoch và eval_model\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\")\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            progress_bar.set_postfix({\"Loss\": loss.item(), \"Accuracy\": correct_predictions.item() / len(dataloader.dataset)})\n",
    "    return total_loss / len(dataloader), correct_predictions.double() / len(dataloader.dataset), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab9c57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf9ec16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 495/495 [02:35<00:00,  3.19batch/s, Loss=0.119] \n",
      "Evaluating: 100%|██████████| 124/124 [00:14<00:00,  8.68batch/s, Loss=0.487, Accuracy=0.951] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2832\n",
      "Validation Loss: 0.1256, Validation Accuracy: 0.9505\n",
      "Epoch 2/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 495/495 [03:17<00:00,  2.51batch/s, Loss=0.0807] \n",
      "Evaluating: 100%|██████████| 124/124 [00:17<00:00,  7.29batch/s, Loss=0.406, Accuracy=0.952]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1184\n",
      "Validation Loss: 0.0931, Validation Accuracy: 0.9515\n",
      "Epoch 3/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 495/495 [03:10<00:00,  2.60batch/s, Loss=0.155]  \n",
      "Evaluating: 100%|██████████| 124/124 [00:16<00:00,  7.43batch/s, Loss=0.22, Accuracy=0.957]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0991\n",
      "Validation Loss: 0.0855, Validation Accuracy: 0.9571\n",
      "Epoch 4/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 495/495 [02:53<00:00,  2.86batch/s, Loss=0.112]  \n",
      "Evaluating: 100%|██████████| 124/124 [00:14<00:00,  8.39batch/s, Loss=0.52, Accuracy=0.958]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0920\n",
      "Validation Loss: 0.0845, Validation Accuracy: 0.9581\n",
      "Epoch 5/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 495/495 [02:49<00:00,  2.92batch/s, Loss=0.173]   \n",
      "Evaluating: 100%|██████████| 124/124 [00:15<00:00,  7.82batch/s, Loss=0.586, Accuracy=0.958]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0880\n",
      "Validation Loss: 0.0833, Validation Accuracy: 0.9576\n",
      "Epoch 6/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 495/495 [02:49<00:00,  2.92batch/s, Loss=0.0619]  \n",
      "Evaluating: 100%|██████████| 124/124 [00:14<00:00,  8.33batch/s, Loss=0.179, Accuracy=0.959]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0856\n",
      "Validation Loss: 0.0779, Validation Accuracy: 0.9591\n",
      "Epoch 7/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 495/495 [02:49<00:00,  2.92batch/s, Loss=0.0573]  \n",
      "Evaluating: 100%|██████████| 124/124 [00:16<00:00,  7.54batch/s, Loss=0.149, Accuracy=0.959]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0905\n",
      "Validation Loss: 0.0783, Validation Accuracy: 0.9591\n",
      "Epoch 8/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 495/495 [03:31<00:00,  2.34batch/s, Loss=0.0554]  \n",
      "Evaluating: 100%|██████████| 124/124 [00:18<00:00,  6.87batch/s, Loss=0.188, Accuracy=0.959]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0840\n",
      "Validation Loss: 0.0786, Validation Accuracy: 0.9591\n",
      "Epoch 9/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 495/495 [02:44<00:00,  3.01batch/s, Loss=0.000331]\n",
      "Evaluating: 100%|██████████| 124/124 [00:14<00:00,  8.84batch/s, Loss=0.129, Accuracy=0.959]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0840\n",
      "Validation Loss: 0.0848, Validation Accuracy: 0.9591\n",
      "Epoch 10/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 312/495 [01:38<00:59,  3.08batch/s, Loss=0.0785]  "
     ]
    }
   ],
   "source": [
    "# Huấn luyện mô hình\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "    val_loss, val_acc, _, _ = eval_model(model, test_dataloader, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Đánh giá mô hình trên tập kiểm tra\n",
    "test_loss, test_acc, test_labels, test_preds = eval_model(model, test_dataloader, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Sử dụng hàm evaluate_model để tính các chỉ số đánh giá\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "evaluate_model(test_labels, test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
